{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yopjHEMaFspp"
   },
   "source": [
    "## Problem statement:\n",
    "Build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "slpcUW2nGgri",
    "outputId": "61c791e2-7327-4d0a-fbac-bcde19f71105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i8Nw3IaWGo-2"
   },
   "outputs": [],
   "source": [
    "#unzip the dataset\n",
    "!unzip \"/content/gdrive/MyDrive/SkinCancerDataset.zip\" > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ttDOGhY3FqH0"
   },
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "C_C-UQdZG2SN"
   },
   "outputs": [],
   "source": [
    "# Defining the path for train and test images\n",
    "data_dir_train = pathlib.Path(\"/content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\")\n",
    "data_dir_test = pathlib.Path(\"/content/Skin cancer ISIC The International Skin Imaging Collaboration/Test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FWusnolHNZ8",
    "outputId": "1fb0209c-f58f-45c6-db86-2a803ca90698"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2239\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "# Count the number of image in Train and Test directory\n",
    "# Using the glob to retrieve files/pathnames matching a specified pattern.\n",
    "\n",
    "#Train Image count\n",
    "image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n",
    "print(image_count_train)\n",
    "\n",
    "#Test Image count\n",
    "image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n",
    "print(image_count_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQ6jAHi0Hesp"
   },
   "source": [
    "**Data Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 878
    },
    "id": "UfJ9Ebp_HYGj",
    "outputId": "cc11794d-56ab-40be-de95-3da07d510f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2239 files belonging to 9 classes.\n"
     ]
    },
        },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize one instance of all the class present in the dataset.\n",
    "\n",
    "#image_dataset_from_directory() will return a tf.data.Dataset that yields batches of images from the subdirectories.\n",
    "#label_mode is categorial, the labels are a float32 tensor of shape (batch_size, num_classes), representing a one-hot encoding of the class index.\n",
    "image_dataset = tf.keras.preprocessing.image_dataset_from_directory(data_dir_train,batch_size=32,image_size=(180,180),\n",
    "                                                                    label_mode='categorical',seed=123)\n",
    "\n",
    "#all the classes of Skin Cancer\n",
    "class_names = image_dataset.class_names\n",
    "\n",
    "#Dictionary to store the path of image as per the class\n",
    "files_path_dict = {}\n",
    "\n",
    "for c in class_names:\n",
    "    files_path_dict[c] = list(map(lambda x:str(data_dir_train)+'/'+c+'/'+x,os.listdir(str(data_dir_train)+'/'+c)))\n",
    "    \n",
    "#Visualize image \n",
    "plt.figure(figsize=(15,15))\n",
    "index = 0\n",
    "for c in class_names:\n",
    "    path_list = files_path_dict[c][:1]\n",
    "    index += 1\n",
    "    plt.subplot(3,3,index)\n",
    "    plt.imshow(load_img(path_list[0],target_size=(180,180)))\n",
    "    plt.title(c)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tagoG7DBRnVq"
   },
   "source": [
    "**Visualize distribution of classes in the training dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "TzzxmsPCPgO1",
    "outputId": "3fdd1f48-6179-47eb-b978-4a0e1be185c7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>No. of Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>squamous cell carcinoma</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dermatofibroma</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actinic keratosis</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pigmented benign keratosis</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nevus</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vascular lesion</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>seborrheic keratosis</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>basal cell carcinoma</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>melanoma</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Class  No. of Image\n",
       "0     squamous cell carcinoma           181\n",
       "1              dermatofibroma            95\n",
       "2           actinic keratosis           114\n",
       "3  pigmented benign keratosis           462\n",
       "4                       nevus           357\n",
       "5             vascular lesion           139\n",
       "6        seborrheic keratosis            77\n",
       "7        basal cell carcinoma           376\n",
       "8                    melanoma           438"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def class_distribution_count(directory):\n",
    "    \n",
    "    #count number of image in each classes\n",
    "    count= []\n",
    "    for path in pathlib.Path(directory).iterdir():\n",
    "        if path.is_dir():\n",
    "            count.append(len([name for name in os.listdir(path)\n",
    "                               if os.path.isfile(os.path.join(path, name))]))\n",
    "    \n",
    "    #name of the classes\n",
    "    sub_directory = [name for name in os.listdir(directory)\n",
    "                    if os.path.isdir(os.path.join(directory, name))]\n",
    "    \n",
    "    #return dataframe with image count and class.\n",
    "    return pd.DataFrame(list(zip(sub_directory,count)),columns =['Class', 'No. of Image'])\n",
    "\n",
    "df = class_distribution_count(data_dir_train)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "rfifhsqfSPSe",
    "outputId": "00f8dfb7-b61e-464f-dfc6-1d6e192ab5a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f74b43b9390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize the Number of image in each class.\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=\"No. of Image\", y=\"Class\", data=df,\n",
    "            label=\"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5aGnAiNSeUZ"
   },
   "source": [
    "There is a class imbalance to solve this using a python package  Augmentor (https://augmentor.readthedocs.io/en/master/) to add more samples across all classes so that none of the classes have very few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vt7hTjj9SVNJ",
    "outputId": "bd90543a-1fc0-409a-c95c-b52ccea08838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Augmentor\n",
      "  Downloading Augmentor-0.2.8-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.9.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (4.62.3)\n",
      "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (7.1.2)\n",
      "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (0.16.0)\n",
      "Installing collected packages: Augmentor\n",
      "Successfully installed Augmentor-0.2.8\n"
     ]
    }
   ],
   "source": [
    "#install Augmentor\n",
    "!pip install Augmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGjc-ATtSx6a",
    "outputId": "bcfd00c7-08bc-44b9-84b0-e67172d076bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 114 image(s) found.\n",
      "Output directory set to /content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/actinic keratosis/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x450 at 0x7F75338535D0>: 100%|██████████| 500/500 [00:19<00:00, 26.19 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 376 image(s) found.\n",
      "Output directory set to /content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/basal cell carcinoma/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7F758429DED0>: 100%|██████████| 500/500 [00:19<00:00, 25.43 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 95 image(s) found.\n",
      "Output directory set to /content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/dermatofibroma/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x450 at 0x7F75336B3710>: 100%|██████████| 500/500 [00:19<00:00, 25.83 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 438 image(s) found.\n",
      "Output directory set to /content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/melanoma/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=1024x768 at 0x7F74B3A78B90>: 100%|██████████| 500/500 [01:33<00:00,  5.34 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 357 image(s) found.\n",
      "Output directory set to /content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/nevus/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=576x768 at 0x7F75338C8890>: 100%|██████████| 500/500 [01:36<00:00,  5.19 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 462 image(s) found.\n",
      "Output directory set to /content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/pigmented benign keratosis/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x450 at 0x7F74AE7FFB10>: 100%|██████████| 500/500 [00:20<00:00, 24.64 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 77 image(s) found.\n",
      "Output directory set to /content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/seborrheic keratosis/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=1024x768 at 0x7F74B3B58BD0>: 100%|██████████| 500/500 [00:47<00:00, 10.46 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 181 image(s) found.\n",
      "Output directory set to /content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/squamous cell carcinoma/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x450 at 0x7F75336EEF90>: 100%|██████████| 500/500 [00:19<00:00, 25.78 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 139 image(s) found.\n",
      "Output directory set to /content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/vascular lesion/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7F7533950C50>: 100%|██████████| 500/500 [00:19<00:00, 25.74 Samples/s]\n"
     ]
    }
   ],
   "source": [
    "path_to_training_dataset=\"/content/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\"\n",
    "import Augmentor\n",
    "for i in class_names:\n",
    "    p = Augmentor.Pipeline(path_to_training_dataset + i)\n",
    "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
    "    p.sample(500)  #Adding 500 samples per class to make sure that none of the classes are sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Mk8nOl1Uo-j",
    "outputId": "66cb37d1-58f7-48fb-84c0-386ac200593f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n"
     ]
    }
   ],
   "source": [
    "#Count total number of image generated by Augmentor.\n",
    "image_count_train = len(list(data_dir_train.glob('*/output/*.jpg')))\n",
    "print(image_count_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAAYk_frl3ag"
   },
   "source": [
    "**Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "snX47u7G6lH2",
    "outputId": "d5fda397-8509-461d-ce7a-985c8e1a2755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6739 files belonging to 9 classes.\n",
      "Using 5392 files for training.\n"
     ]
    }
   ],
   "source": [
    "# train dataset \n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir_train, batch_size=32,\n",
    "                                                               image_size=(180,180), label_mode='categorical',\n",
    "                                                               seed=123,subset=\"training\",\n",
    "                                                               validation_split=0.2)\n",
    "\n",
    "#label_mode is categorial, the labels are a float32 tensor of shape (batch_size, num_classes),\n",
    "#representing a one-hot encoding of the class index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2y-MuNHc6w9l",
    "outputId": "d2534817-6e95-4cd5-c094-1076cf9c4ecd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6739 files belonging to 9 classes.\n",
      "Using 1347 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# validation dataset \n",
    "val_ds =tf.keras.preprocessing.image_dataset_from_directory(data_dir_train,batch_size=32,\n",
    "                                                            image_size=(180,180), label_mode='categorical',\n",
    "                                                            seed=123,subset=\"validation\",\n",
    "                                                            validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "code",
    "id": "B43gW84Ao5cy"
   },
   "outputs": [],
   "source": [
    "#tf.data.experimental.AUTOTUNE defines appropriate number of processes that are free for working.\n",
    "\n",
    "#`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch.\n",
    "\n",
    "#`Dataset.prefetch()` overlaps data preprocessing and model execution while training.\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYwPwKFEEDtJ",
    "outputId": "42adba40-b1e1-467d-d411-5d67abc1eca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rescaling (Rescaling)        (None, 180, 180, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 178, 178, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 89, 89, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 87, 87, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 41, 41, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               6553728   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 6,648,137\n",
      "Trainable params: 6,648,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN Model Architecture\n",
    "\n",
    "#Sequential allows you to create models layer-by-layer  \n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.experimental.preprocessing.Rescaling(1./255,input_shape=(180,180,3)))   #Rescaling Layer\n",
    "\n",
    "#First Convulation layer\n",
    "model.add(layers.Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#Second Convulation Layer\n",
    "model.add(layers.Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#Third Convulation Layer\n",
    "model.add(layers.Conv2D(128,kernel_size=(3,3),activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#Dropout layer with 50% Fraction of the input units to drop.\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "#Flatten Layer\n",
    "##Keras.layers.flatten function flattens the multi-dimensional input tensors into a single dimension.\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "#Dense Layer\n",
    "model.add(layers.Dense(128,activation='relu'))\n",
    "\n",
    "#Dropout layer with 25% Fraction of the input units to drop.\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "#Dense Layer with softmax activation function.\n",
    "#Softmax is an activation function that scales numbers/logits into probabilities.\n",
    "model.add(layers.Dense(len(class_names),activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jr56VzzvtsLA",
    "outputId": "48208794-41db-4ec6-f664-a6f4695c5d3f"
   },
  
   ],
   "source": [
    "# vizualizing the model \n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "woAdr-mXqRKE"
   },
   "outputs": [],
   "source": [
    "#Compile the Model\n",
    "\n",
    "#Adam optimization: is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "#categorical_crossentropy: Used as a loss function for multi-class classification model where there are two or more output labels.\n",
    "\n",
    "model.compile(optimizer=\"Adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "#ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights (in a checkpoint file) at some interval,\n",
    "#so the model or weights can be loaded later to continue the training from the state saved.\n",
    "checkpoint = ModelCheckpoint(\"model.h5\",monitor=\"val_accuracy\",save_best_only=True,mode=\"auto\",verbose=1)\n",
    "\n",
    "#Stop training when a monitored metric has stopped improving.\n",
    "earlystop = EarlyStopping(monitor=\"val_accuracy\",patience=5,mode=\"auto\",verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnMSvuzkqx-q",
    "outputId": "797bcb80-d59b-4d1b-8bdc-e58b93199004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "169/169 [==============================] - 69s 102ms/step - loss: 1.8030 - accuracy: 0.3166 - val_loss: 1.5269 - val_accuracy: 0.4321\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.43207, saving model to model.h5\n",
      "Epoch 2/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 1.4552 - accuracy: 0.4518 - val_loss: 1.3301 - val_accuracy: 0.5308\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.43207 to 0.53081, saving model to model.h5\n",
      "Epoch 3/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 1.3019 - accuracy: 0.5109 - val_loss: 1.2471 - val_accuracy: 0.5434\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.53081 to 0.54343, saving model to model.h5\n",
      "Epoch 4/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 1.1884 - accuracy: 0.5555 - val_loss: 1.0393 - val_accuracy: 0.6288\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.54343 to 0.62880, saving model to model.h5\n",
      "Epoch 5/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 1.0936 - accuracy: 0.5920 - val_loss: 1.0634 - val_accuracy: 0.6006\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.62880\n",
      "Epoch 6/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 0.9737 - accuracy: 0.6422 - val_loss: 0.8937 - val_accuracy: 0.6711\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.62880 to 0.67112, saving model to model.h5\n",
      "Epoch 7/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 0.8609 - accuracy: 0.6908 - val_loss: 1.0064 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.67112\n",
      "Epoch 8/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 0.8109 - accuracy: 0.7012 - val_loss: 0.8194 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.67112 to 0.70156, saving model to model.h5\n",
      "Epoch 9/20\n",
      "169/169 [==============================] - 12s 68ms/step - loss: 0.7206 - accuracy: 0.7350 - val_loss: 0.7097 - val_accuracy: 0.7513\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.70156 to 0.75130, saving model to model.h5\n",
      "Epoch 10/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 0.6686 - accuracy: 0.7532 - val_loss: 0.7351 - val_accuracy: 0.7602\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.75130 to 0.76021, saving model to model.h5\n",
      "Epoch 11/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 0.6022 - accuracy: 0.7695 - val_loss: 0.7016 - val_accuracy: 0.7580\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.76021\n",
      "Epoch 12/20\n",
      "169/169 [==============================] - 12s 68ms/step - loss: 0.5660 - accuracy: 0.7897 - val_loss: 0.7897 - val_accuracy: 0.7290\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.76021\n",
      "Epoch 13/20\n",
      "169/169 [==============================] - 12s 68ms/step - loss: 0.5252 - accuracy: 0.8021 - val_loss: 0.7509 - val_accuracy: 0.7706\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.76021 to 0.77060, saving model to model.h5\n",
      "Epoch 14/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 0.4755 - accuracy: 0.8251 - val_loss: 0.6387 - val_accuracy: 0.8062\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.77060 to 0.80624, saving model to model.h5\n",
      "Epoch 15/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 0.4638 - accuracy: 0.8259 - val_loss: 0.6336 - val_accuracy: 0.7751\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.80624\n",
      "Epoch 16/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 0.4002 - accuracy: 0.8453 - val_loss: 0.5632 - val_accuracy: 0.8040\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.80624\n",
      "Epoch 17/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 0.4229 - accuracy: 0.8399 - val_loss: 0.5827 - val_accuracy: 0.8211\n",
      "\n",
      "Epoch 00017: val_accuracy improved from 0.80624 to 0.82108, saving model to model.h5\n",
      "Epoch 18/20\n",
      "169/169 [==============================] - 12s 68ms/step - loss: 0.4143 - accuracy: 0.8420 - val_loss: 0.5611 - val_accuracy: 0.8114\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.82108\n",
      "Epoch 19/20\n",
      "169/169 [==============================] - 12s 68ms/step - loss: 0.3800 - accuracy: 0.8542 - val_loss: 0.5537 - val_accuracy: 0.8426\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.82108 to 0.84261, saving model to model.h5\n",
      "Epoch 20/20\n",
      "169/169 [==============================] - 12s 69ms/step - loss: 0.4168 - accuracy: 0.8477 - val_loss: 0.5865 - val_accuracy: 0.8129\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.84261\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 20\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs,callbacks=[checkpoint,earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "weY4vnJ3rK4u",
    "outputId": "614f41b2-133a-4f84-ff5b-b1ad79740048"
   },
   
   "source": [
    "# Plot the training curves\n",
    "\n",
    "epochs_range = range(earlystop.stopped_epoch+1)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "#Plot Model Accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel(epochs_range)\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "#Plot Model Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel(epochs_range)\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4kxzWIc7fck"
   },
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "Uc9u92x28N4m",
    "outputId": "d16d0333-b6da-4fac-dfb9-53e0a2ac1006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Class basal cell carcinoma\n",
      "Predictive Class basal cell carcinoma\n"
     ]
    },
    
   "source": [
    "from glob import glob\n",
    "Test_image_path = os.path.join(data_dir_test, class_names[1], '*')\n",
    "Test_image = glob(Test_image_path)\n",
    "Test_image = load_img(Test_image[-1],target_size=(180,180,3))\n",
    "plt.imshow(Test_image)\n",
    "plt.grid(False)\n",
    "\n",
    "img = np.expand_dims(Test_image,axis=0)\n",
    "pred = model.predict(img)\n",
    "pred = np.argmax(pred)\n",
    "pred_class = class_names[pred]\n",
    "print(\"Actual Class \"+ class_names[1] +'\\n'+ \"Predictive Class \"+pred_class )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Melanoma_SkinCancer_Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
